{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AspirinZJ/AspirinZJ.github.io/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXsoXK5GwybN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount google drive\n",
        "DRIVE_MOUNT='/content/gdrive'\n",
        "drive.mount(DRIVE_MOUNT)\n",
        "\n",
        "\n",
        "# create folder to write data to\n",
        "CIS680_FOLDER=os.path.join(DRIVE_MOUNT, 'My Drive', 'CIS680_2019')\n",
        "HOMEWORK_FOLDER=os.path.join(CIS680_FOLDER, 'HW3b')\n",
        "os.makedirs(HOMEWORK_FOLDER, exist_ok=True)\n",
        "MODEL_PATH = os.path.join(HOMEWORK_FOLDER, 'checkpoint.pth')\n",
        "MASK_MODEL_PATH = os.path.join(HOMEWORK_FOLDER, 'checkpoint-mask.pth')\n",
        "\n",
        "# bootstrap environment into place\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "import io\n",
        "import os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "def download_file(fn, file_id):\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    downloaded = io.BytesIO()\n",
        "    downloader = MediaIoBaseDownload(downloaded, request)\n",
        "    done = False\n",
        "    while done is False:\n",
        "        # _ is a placeholder for a progress object that we ignore.\n",
        "        # (Our file is small, so we skip reporting progress.)\n",
        "        _, done = downloader.next_chunk()\n",
        "    \n",
        "    downloaded.seek(0)\n",
        "\n",
        "    folder = fn.split('/')\n",
        "    if len(folder) > 1:\n",
        "        os.makedirs(folder[0], exist_ok=True)\n",
        "\n",
        "    with open(fn, 'wb') as f:\n",
        "        f.write(downloaded.read())\n",
        "\n",
        "id_to_fn = {\n",
        "'1uBWazGxSZgWs70JjSWBu-KZwy5sAcxLh': 'hw3_mycocodata_bboxes_comp_zlib.npy',\n",
        "'18Bh2qwVwdDwu7JK_plrnpAz9KjA5gWkv': 'hw3_mycocodata_img_comp_zlib.h5',\n",
        "'1K4eZGmbW0peZvcSRpgJeCSuI6A6PWZYz': 'hw3_mycocodata_labels_comp_zlib.npy',\n",
        "'1xIzQrhWrJeid1J8YLFMt8yigFVDA-N8a': 'hw3_mycocodata_mask_comp_zlib.h5',\n",
        "'1fZY-tP2MuUYCA7Ur0C0zGTfEBs1XswGQ': 'checkpoint680.pth',\n",
        "}\n",
        "\n",
        "# download all files into the vm\n",
        "for fid, fn in id_to_fn.items():\n",
        "    print(\"Downloading %s from %s\" % (fn, fid))\n",
        "    download_file(fn, fid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfbcQy7Aw3KX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch and torchvision imports\n",
        "import torch\n",
        "import torchvision\n",
        "import h5py\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import random\n",
        "from sklearn.metrics import average_precision_score\n",
        "import colorsys\n",
        "import gc\n",
        "from torchvision.models.detection.image_list import ImageList \n",
        "from torchvision import ops\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "use_cuda = True\n",
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8TgbrOpxlrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(image, masks_per_img, bboxes_per_img):\n",
        "    '''\n",
        "    resize, pad and normalize\n",
        "    parameters:\n",
        "        image: np.array([3, 300, 400])\n",
        "        masks: np.array(n_mask_per_image, 300, 400)\n",
        "        bboxes: np.array(n_boxes_per_image, 4)\n",
        "    returns:\n",
        "        image_transformed: torch.tensor([3, 800, 1088])\n",
        "        masks_transformed: torch.tensor([n_masks, 800, 1088])\n",
        "        bbox_resized_pad: torch.tensor[n_boxes_per_image, 4]\n",
        "    '''\n",
        "\n",
        "    # Pre-process image, masks and bboxes\n",
        "    img_transforms = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((800, 1066)),\n",
        "        transforms.Pad((11, 0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    mask_transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((800, 1066)),\n",
        "        transforms.Pad((11, 0))\n",
        "        ])\n",
        "\n",
        "    # 1. Transform image\n",
        "    img_np = np.transpose(image.astype(np.uint8), (1, 2, 0))\n",
        "    image_transformed = img_transforms(img_np)\n",
        "\n",
        "    # 2. Transform masks\n",
        "    masks_transformed = torch.tensor([])\n",
        "    for j in range(masks_per_img.shape[0]):\n",
        "        mask = masks_per_img[j].astype(np.uint8)\n",
        "        mask_transformed = torch.from_numpy(np.array(mask_transform(mask))).type(torch.float)\n",
        "        mask_transformed = mask_transformed.unsqueeze(0)\n",
        "        masks_transformed = torch.cat((masks_transformed, mask_transformed), dim=0)\n",
        "\n",
        "    # 3. Transform bounding boxes\n",
        "    bbox_resized = bboxes_per_img.copy()\n",
        "    scale = 800 / 300\n",
        "    bbox_resized[:,0:2] = bbox_resized[:,0:2] * scale\n",
        "    bbox_resized[:, 2:] = bbox_resized[:,2:] * scale\n",
        "    bbox_resized_pad = torch.from_numpy(bbox_resized + 11)\n",
        "\n",
        "    return image_transformed, masks_transformed, bbox_resized_pad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOni2oZSi8dY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um2qqIukxriO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HW3Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.bboxes = np.load(path[0], allow_pickle=True, encoding='latin1')\n",
        "        self.images = h5py.File(path[1], 'r')['data']\n",
        "        self.labels = np.load(path[2], allow_pickle=True, encoding='latin1')\n",
        "        self.masks_original = h5py.File(path[3], 'r')['data']\n",
        "        self.masks = []\n",
        "        self.k = 0          # total masks in dataset\n",
        "        gc.collect()\n",
        "\n",
        "        '''\n",
        "        # Itrate over all bounding boxes in the dataset\n",
        "        # Divide elements in mask\n",
        "        '''\n",
        "        # iterate over each image\n",
        "        for idx, bboxes_per_img in enumerate(self.bboxes):\n",
        "\n",
        "            # iterate over each bbox\n",
        "            for i in range(self.bboxes[idx].shape[0]):\n",
        "                if i == 0:\n",
        "                    mask = self.masks_original[self.k]\n",
        "                    mask = np.expand_dims(mask, axis=0)\n",
        "                else:\n",
        "                    mask_cat = np.expand_dims(self.masks_original[self.k], axis=0)\n",
        "                    mask = np.concatenate((mask, mask_cat), axis = 0)\n",
        "                self.k += 1\n",
        "                \n",
        "            # Assign masks\n",
        "            self.masks.append(mask)     # as a list\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.images.shape[0]\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index, _ = torch.sort(index)\n",
        "            index = index.tolist()\n",
        "            \n",
        "        # Pre-process the images, masks and bounding boxes\n",
        "        img_trans, mask_trans, bbox_trans = preprocess(self.images[index], self.masks[index], self.bboxes[index])\n",
        "        data = {'image': img_trans, \n",
        "                'target': {'label': torch.from_numpy(self.labels[index]), 'bbox': bbox_trans, \n",
        "                           'mask': mask_trans}}\n",
        "        return data\n",
        "\n",
        "\n",
        "path = [\"hw3_mycocodata_bboxes_comp_zlib.npy\", 'hw3_mycocodata_img_comp_zlib.h5', 'hw3_mycocodata_labels_comp_zlib.npy', 'hw3_mycocodata_mask_comp_zlib.h5']\n",
        "dataset = HW3Dataset(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfQdTL_3yCSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i=1\n",
        "print(\"image size before transformed: \", dataset.images[i].shape)   \n",
        "print(\"image size after transformed: \", dataset[i]['image'].shape)\n",
        "print(\"mask size after transformed: \", dataset[i]['target']['mask'].shape)\n",
        "print(\"label size:\", dataset[i]['target']['label'].shape)\n",
        "\n",
        "\"\"\"# Plot Dataset\"\"\"\n",
        "\n",
        "def random_colors(N, bright=True):\n",
        "    \"\"\"\n",
        "    Generate random colors.\n",
        "    To get visually distinct colors, generate them in HSV space then\n",
        "    convert to RGB.\n",
        "    \"\"\"\n",
        "    brightness = 1.0 if bright else 0.7\n",
        "    hsv = [(i / N, 1, brightness) for i in range(N)]\n",
        "    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n",
        "    random.shuffle(colors)\n",
        "    return colors\n",
        "\n",
        "\n",
        "def apply_mask(image, mask, color, alpha=0.5):\n",
        "    \"\"\"Apply the given mask to the image.\n",
        "    \"\"\"\n",
        "    for c in range(3):\n",
        "        image[:, :, c] = np.where(mask == 1,\n",
        "                                  image[:, :, c] *\n",
        "                                  (1 - alpha) + alpha * color[c] * 255,\n",
        "                                  image[:, :, c])\n",
        "    return image\n",
        "\n",
        "def draw_bboxes(image, new_rpn, ax):\n",
        "    '''\n",
        "    image: tensor(3, 800, 1088)\n",
        "    new_rpn: tensor[n, 9], n is the number of boxes on one image\n",
        "    '''\n",
        "    bboxes = new_rpn[:, :4]\n",
        "    label = new_rpn[:, 4]\n",
        "    image = np.transpose(image.numpy(), (1, 2, 0))\n",
        "    image_convert = (image - image.min()) * (1/(image.max() - image.min()) * 255)\n",
        "    ax.imshow(image_convert.astype(np.uint8))\n",
        "    \n",
        "    for i in range(bboxes.shape[0]):\n",
        "        # Bounding box\n",
        "        x1, y1, x2, y2 = bboxes[i,:]\n",
        "        w = x2 - x1\n",
        "        h = y2 - y1\n",
        "\n",
        "        if label[i] > 0:\n",
        "            color = 'r'\n",
        "        else:\n",
        "            color = 'g'\n",
        "\n",
        "        # ax = plt.gca()\n",
        "        rect = patches.Rectangle((x1,y1),w,h,linewidth=2,edgecolor=color,facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "\n",
        "def display_instance(data, plt, scores=None, masks=None, labels=None, image=None, boxes=None):\n",
        "    '''\n",
        "    data: 1*{'image', 'target'}\n",
        "    masks: [100, 800, 1088]\n",
        "    '''\n",
        "    if data:\n",
        "        image = data['image'].numpy()\n",
        "        image = np.transpose(image, (1, 2, 0))\n",
        "        boxes = data['target']['bbox'].numpy()\n",
        "        masks = data['target']['mask'].numpy() \n",
        "        labels = data['target']['label'].numpy()\n",
        "\n",
        "    else:\n",
        "        image = np.transpose(image.cpu().numpy(), (1, 2, 0))\n",
        "        boxes = boxes.cpu().numpy()\n",
        "        masks = masks.cpu().numpy()\n",
        "        labels = labels.cpu().numpy()\n",
        "    \n",
        "    class_names = {1: \"vehicle\", 2: 'people', 3: 'animal'}\n",
        "    \n",
        "    # Number of instances\n",
        "    N = boxes.shape[0]\n",
        "    if not N:\n",
        "        print(\"\\n*** No instances to display *** \\n\")\n",
        "    else:\n",
        "        assert boxes.shape[0] == masks.shape[0] == labels.shape[0]\n",
        "\n",
        "    # Generate random colors\n",
        "    colors = random_colors(N)\n",
        "\n",
        "    image_convert = (image - image.min()) * (1/(image.max() - image.min()) * 255)\n",
        "    masked_image = image_convert.astype(np.uint32).copy()\n",
        "\n",
        "    # Plot bounding boxes\n",
        "    for i in range(N):\n",
        "        color = colors[i]\n",
        "        # Bounding box\n",
        "        x1, y1, x2, y2 = boxes[i,:]\n",
        "        w = x2 - x1\n",
        "        h = y2 - y1\n",
        "\n",
        "        if labels[i] == 1:    # vehicles\n",
        "            color_box = 'r'\n",
        "        elif labels[i] == 2:  # people\n",
        "            color_box = 'b'  \n",
        "        elif labels[i] == 3:  # animals\n",
        "            color_box = 'g'\n",
        "\n",
        "        ax = plt.gca()\n",
        "        rect = patches.Rectangle((x1,y1),w,h,linewidth=2,edgecolor=color_box,facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Label\n",
        "        label = labels[i]\n",
        "        score = scores[i] if scores is not None else None\n",
        "        label_name = class_names[label]\n",
        "        caption = \"{} {:.3f}\".format(label_name, score) if score else label_name\n",
        "        ax.text(x1, y1, caption,\n",
        "                color='w', size=12, backgroundcolor=\"b\")\n",
        "\n",
        "\n",
        "        # Mask\n",
        "        mask = masks[i].astype(np.uint32)\n",
        "        masked_image = apply_mask(masked_image, mask, color)\n",
        "\n",
        "        ax.imshow(masked_image.astype(np.uint8))\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "for i in range(0, 10):\n",
        "    plt.figure(i)\n",
        "    data = dataset[i]\n",
        "    display_instance(data, plt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjaax-YjyePy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_iou(boxA, boxB):\n",
        "    '''\n",
        "    compute IOU of two boxes\n",
        "    box1, box2:(x1, y1, x2, y2)\n",
        "    '''\n",
        "\n",
        "    # determine the (x, y)-coordinates of the intersection rectangle\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "\n",
        "    # compute the area of intersection rectangle\n",
        "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
        "\n",
        "    # compute the area of both the prediction and ground-truth\n",
        "    # rectangles\n",
        "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
        "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
        "\n",
        "    # compute the intersection over union by taking the intersection\n",
        "    # area and dividing it by the sum of prediction + ground-truth\n",
        "    # areas - the interesection area\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "\n",
        "    # return the intersection over union value\n",
        "    return iou\n",
        "\n",
        "\n",
        "def assign_gt_labels(rpnout, bboxes, labels, iou=0.5):\n",
        "    '''\n",
        "    For each proposal find the IOU with the ground truth boxes. \n",
        "    1) If IOU is smaller than let's say 0.5 you can consider this background. \n",
        "    If it is >0.5 this proposal is assigned the label of the corresponding bounding box \n",
        "    for the classifier (and the coordinates for the regressor). \n",
        "    2) The proposal that has the maximum IOU with a bounding box is assigned \n",
        "    by this bounding box as target. \n",
        "    params:\n",
        "        rpnout: torch.tensor([1000, 4]) in form of [x1,y1,x2,y2]\n",
        "        bboxes: torch.tensor([n_boxes, 4]) in form of [x1,y1,x2,y2]\n",
        "        labels: torch.tensor(n_boxes)\n",
        "    return:\n",
        "        new_proposals: torch.tensor([1000, 9]) in form of [x1, y1, x2, y2, gt_label, gt_x1, gt_y1, gt_x2, gt_y2]\n",
        "    '''\n",
        "    rpn_num = rpnout.shape[0]   # 1000\n",
        "    n_bbox = bboxes.shape[0]    # number of boxes\n",
        "    iou_max = [0.0] * n_bbox\n",
        "    iou_max_idx = [0] * n_bbox\n",
        "    new_proposals = torch.tensor([]).to(device)\n",
        "    \n",
        "    # iterate over each rpn output \n",
        "    for i in range(rpn_num):\n",
        "        rpn = rpnout[i].clone()\n",
        "        new_rpn = torch.zeros(9,).to(device)\n",
        "        new_rpn[:4] = rpn\n",
        "        # compare with every ground truth bbox\n",
        "        non_iou_count = 0\n",
        "        for bbox_idx, bbox in enumerate(bboxes):\n",
        "\n",
        "            # compute the IOU of ground truth box and rpn box\n",
        "            iou = compute_iou(bbox, rpn)\n",
        "\n",
        "            # if iou>0.5, assign the label and coordinates\n",
        "            if iou >= 0.5:\n",
        "                new_rpn[4] = labels[bbox_idx]\n",
        "                new_rpn[5:] = bbox\n",
        "\n",
        "            # keep track of the highest IOU of that bounding box\n",
        "            if iou > iou_max[bbox_idx]:\n",
        "                iou_max[bbox_idx] = iou\n",
        "                iou_max_idx[bbox_idx] = i\n",
        "\n",
        "        new_proposals = torch.cat((new_proposals, new_rpn.unsqueeze(0)), dim=0)\n",
        "\n",
        "    # After iterate all rpn boxes, assign the max iou\n",
        "    for bbox_idx, bbox in enumerate(bboxes):\n",
        "        new_proposals[iou_max_idx[bbox_idx], 4] = labels[bbox_idx]\n",
        "        new_proposals[iou_max_idx[bbox_idx], 5:] =  bbox\n",
        "\n",
        "    # print(\"shape of new proposals: \", new_proposals.shape)\n",
        "    return new_proposals\n",
        "\n",
        "\n",
        "def subsample(new_rpn, equal=True):\n",
        "    '''\n",
        "    After this assignment subsample negative (background) and positive regions \n",
        "    in a ratio as close to 1:1 as possible.\n",
        "    params:\n",
        "        new_rpn: torch.tensor([1000, 9]) in form of [x1, y1, x2, y2, gt_label, gt_x1, gt_y1, gt_x2, gt_y2]\n",
        "        equal: True to subsample 1:1, False to subsmaple to 128\n",
        "    returns:\n",
        "        subsampled tensors\n",
        "    '''\n",
        "    if new_rpn.shape[1] > 4:\n",
        "        # positive and negative masks\n",
        "        positive_idx = torch.where(new_rpn[:, 4] > 0)      # label > 0\n",
        "        positive_num = positive_idx[0].shape[0]\n",
        "        # print(\"number of positives: \", positive_num)\n",
        "        negative_idx = torch.where(new_rpn[:, 4] == 0)\n",
        "        \n",
        "        # subsample negative indices to 1:1 or 128 batch size\n",
        "        if equal:\n",
        "            sampled_idx_idx = random.sample(range(negative_idx[0].shape[0]), positive_num)\n",
        "        else:\n",
        "            sampled_idx_idx = random.sample(range(negative_idx[0].shape[0]), 128 - positive_num)\n",
        "\n",
        "        sampled_neg_idx = negative_idx[0][sampled_idx_idx]\n",
        "        # print(\"negative number: \", sampled_neg_idx.shape[0])\n",
        "        \n",
        "        sampled_idx = torch.cat((positive_idx[0], sampled_neg_idx))\n",
        "        # print(\"total size: \", sampled_idx.shape)\n",
        "        sorted, _ = torch.sort(sampled_idx)\n",
        "        sampled_rpn = new_rpn[sorted]\n",
        "\n",
        "    else:\n",
        "        # positive and negative masks\n",
        "        positive_idx = torch.where(new_rpn > 0)      # label > 0\n",
        "        positive_num = positive_idx[0].shape[0]\n",
        "        print(\"number of positives: \", positive_num)\n",
        "        negative_idx = torch.where(new_rpn == 0)\n",
        "        \n",
        "        # subsample negative indices to 1:1 or 128 batch size\n",
        "        if equal:\n",
        "            sampled_idx_idx = random.sample(range(negative_idx[0].shape[0]), positive_num)\n",
        "        else:\n",
        "            sampled_idx_idx = random.sample(range(negative_idx[0].shape[0]), 128 - positive_num)\n",
        "\n",
        "        sampled_neg_idx = negative_idx[0][sampled_idx_idx]\n",
        "        print(\"negative number: \", sampled_neg_idx.shape[0])\n",
        "        \n",
        "        sampled_idx = torch.cat((positive_idx[0], sampled_neg_idx))\n",
        "        # print(\"total size: \", sampled_idx.shape)\n",
        "        sorted, _ = torch.sort(sampled_idx)\n",
        "        sampled_rpn = new_rpn[sorted]\n",
        "\n",
        "    return sampled_rpn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adg6QZ3MzBNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretrained_model_680(checkpoint_file):\n",
        "\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False)\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    model.to(device)\n",
        "    \n",
        "    backbone = model.backbone\n",
        "    rpn = model.rpn\n",
        "    \n",
        "    checkpoint = torch.load(checkpoint_file)\n",
        "    \n",
        "    backbone.load_state_dict(checkpoint['backbone'])\n",
        "    rpn.load_state_dict(checkpoint['rpn'])\n",
        "    \n",
        "    return backbone, rpn\n",
        "\n",
        "\n",
        "backbone,rpn = pretrained_model_680('checkpoint680.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBT464DfzEl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(data, RPN_plot=False, ROI_plot=False):\n",
        "    '''\n",
        "    The mask loss Lmask is defined only on\n",
        "    positive RoIs. The mask target is the intersection between\n",
        "    an RoI and its associated ground-truth mask.\n",
        "    We produce one mask for each class.\n",
        "\n",
        "    returns:\n",
        "        rpn_aligned_unroll: [sampled_rpns, 256*7*7]\n",
        "        rpn_result: [sampled_rpns, 9] in form of torch.tensor[x1, y1, x2, y2, label, gt_x1, gt_y1, gt_x2, gt_y2]\n",
        "        p2: p2 from RPN network, [1, 256, h/4, w/4]\n",
        "    '''\n",
        "\n",
        "    image = data['image'].unsqueeze(0).to(device)      # image as a FloatTensor with size (n_batch, 3, 800, 1088)\n",
        "    targets = data['target']\n",
        "    masks = targets['mask'].to(device)\n",
        "    bboxes = targets['bbox'].to(device)\n",
        "    labels = targets['label'].to(device)\n",
        "\n",
        "    #########################\n",
        "    # Prepare for box model #\n",
        "    #########################\n",
        "    # Get feature P2\n",
        "    for param in backbone.parameters():                 # freeze the weights\n",
        "        param.requires_grad = False\n",
        "    backout = backbone(image)\n",
        "    p2 = backout[0]                                     # p2: [1, 256, h/4, w/4]\n",
        "\n",
        "    # Get rpn output\n",
        "    im_lis = ImageList(image, [(800,1088)])\n",
        "    rpn.eval()\n",
        "    for param in rpn.parameters():                      # freeze the weights\n",
        "        param.requires_grad = False\n",
        "    rpnout = rpn(im_lis,backout)                        # rpnout[0][0] is size(1000, 4)\n",
        "    proposals = rpnout[0][0]\n",
        "\n",
        "    # Assign ground truth bbox and label to positive rpns\n",
        "    new_rpn = assign_gt_labels(proposals, bboxes, labels)   # [num_rpns, 9]\n",
        "\n",
        "    # Subsample assigned rpns to 128 \n",
        "    rpn_result = subsample(new_rpn, False)              # [sampled_rpns, 9]\n",
        "\n",
        "    ## Visualization\n",
        "    # Plot some proposals\n",
        "    if RPN_plot:\n",
        "        fig = plt.figure(i)   \n",
        "        ax = fig.add_subplot(1, 1, 1)    \n",
        "        draw_bboxes(image.squeeze().cpu(), rpn_result[:50], ax)\n",
        "\n",
        "    if ROI_plot:\n",
        "        n_cols = 5\n",
        "        fig = plt.figure(i, figsize=(60, 60))\n",
        "        ax1 = fig.add_subplot(1,n_cols,1)        \n",
        "        draw_bboxes(image.squeeze().cpu(), rpn_result, ax1)\n",
        "\n",
        "    # Roi Align\n",
        "    rpn_boxes = rpn_result[:, :4]                       # in image coordinate\n",
        "    roi_align = ROIAlign()\n",
        "    aligned = roi_align(p2, [rpn_boxes/4], [7,7])       # [sampled_rpns, 256, 7, 7]\n",
        "    \n",
        "    ## Visualization\n",
        "    # Plot some croped image after Roi Align\n",
        "    if ROI_plot:\n",
        "        aligned_image = roi_align(image, [rpn_boxes], [50,50])\n",
        "        for j in range(2, 6):\n",
        "            image_show = aligned_image[j-2].squeeze().cpu()\n",
        "            image_convert = (image_show - image_show.min()) * (1/(image_show.max() - image_show.min()) * 255)\n",
        "            ax2 = fig.add_subplot(1, n_cols, j)\n",
        "            ax2.imshow(image_convert.permute(1,2,0).type(torch.uint8))\n",
        "\n",
        "    # Unroll roi output into a C*P*P vector\n",
        "    N = aligned.shape[0]\n",
        "    rpn_aligned_unroll = aligned.view(N, -1)            # [sampled_rpns, 256*7*7]\n",
        "\n",
        "\n",
        "    ##########################\n",
        "    # Prepare for mask model #\n",
        "    ##########################\n",
        "\n",
        "    target = torch.zeros(4, 28, 28).to(device)\n",
        "    label_once = []\n",
        "    # crop mask as bbox\n",
        "    for idx in range(bboxes.shape[0]):\n",
        "\n",
        "        # only use the first occurred label\n",
        "        label = labels[idx]\n",
        "        if label in label_once:                             \n",
        "            continue\n",
        "        label_once.append(label)\n",
        "\n",
        "        x1, y1, x2, y2 = bboxes[idx].type(torch.int)        # bbox [4,]\n",
        "        mask = masks[idx]                                   # mask [800, 1088]\n",
        "        mask_cropped = mask[y1:y2, x1:x2]                  \n",
        "        mask_unsquez = mask_cropped.unsqueeze(0)\n",
        "        mask_unsquez = mask_unsquez.unsqueeze(0)\n",
        "\n",
        "        # Resize cropped mask to 28 * 28\n",
        "        mask_resized = F.interpolate(mask_unsquez, (28, 28), mode='bilinear', align_corners=True)   # [1, 1, 28, 28]\n",
        "        positive_mask = torch.where(mask_resized > 0)\n",
        "        mask_resized[positive_mask] = 1\n",
        "        target[label] = mask_resized.squeeze()\n",
        "\n",
        "    mask_target = target.unsqueeze(0).expand(100, -1, -1, -1)\n",
        "    return rpn_aligned_unroll, rpn_result, p2, mask_target  # rpn_result:[sampled_rpns, 9]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NohZeIppzIXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ROIAlign(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, rpn_out, boxes, size=7):\n",
        "        output = ops.roi_align(rpn_out, boxes, size)\n",
        "        return output\n",
        "\n",
        "class Boxes(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Interlayer\n",
        "        self.fc1 = nn.Linear(in_features=7*7*256, out_features=1024)\n",
        "        self.fc2 = nn.Linear(in_features=1024, out_features=1024)\n",
        "\n",
        "        # Classifier\n",
        "        self.fc3 = nn.Linear(in_features=1024, out_features=4)\n",
        "\n",
        "        # Regressor\n",
        "        self.fc4 = nn.Linear(in_features=1024, out_features=4*4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Interlayer\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # Classifier\n",
        "        mrcnn_probs = F.softmax(self.fc3(x), dim=1)\n",
        "\n",
        "        # Regressor\n",
        "        mrcnn_bbox = self.fc4(x)\n",
        "\n",
        "        return [mrcnn_probs, mrcnn_bbox]\n",
        "\n",
        "\n",
        "class Masks(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        # self.bn1 = nn.BatchNorm2d(256, eps=0.001)\n",
        "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        # self.bn2 = nn.BatchNorm2d(256, eps=0.001)\n",
        "        self.conv3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        # self.bn3 = nn.BatchNorm2d(256, eps=0.001)\n",
        "        self.conv4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        # self.bn4 = nn.BatchNorm2d(256, eps=0.001)\n",
        "        self.deconv = nn.ConvTranspose2d(256, 256, kernel_size=2, stride=2)\n",
        "        self.conv6 = nn.Conv2d(256, 4, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        # x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        # x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        # x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        # x = self.bn4(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.deconv(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv6(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pogy7mVBzO_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_boxes(gt_boxes, rpn_boxes, labels):\n",
        "    '''\n",
        "    Create the encoding t* of the ground truth boxes with respect to the proposals\n",
        "    Ground truth boxes are encoded into t∗ vectors and its values are relative to \n",
        "    their corresponding proposal.\n",
        "    t_x = (x − x_a)/w_a \n",
        "    t_y = (y − y_a)/h_a \n",
        "    t_w = log(w/w_a) \n",
        "    t_h = log(h/h_a)\n",
        "    x/y, x_a/y_a are top-left corner of boxes\n",
        "    params:\n",
        "        gt_boxes: [sampled_rpns, 4]\n",
        "        rpn_boxes: [sampled_rpns, 4]\n",
        "        labels: [sampled_rpns, ]\n",
        "    returns:\n",
        "        gt: [sampled_rpns, 4*4]\n",
        "    '''\n",
        "\n",
        "    # Parameterize\n",
        "    w_a = rpn_boxes[:, 2] - rpn_boxes[:, 0]\n",
        "    h_a = rpn_boxes[:, 3] - rpn_boxes[:, 1]\n",
        "    w_gt = gt_boxes[:, 2] - gt_boxes[:, 0] + 0.0000001\n",
        "    h_gt = gt_boxes[:, 3] - gt_boxes[:, 1] + 0.0000001\n",
        "    t_x = (gt_boxes[:, 0] - rpn_boxes[:, 0]) / w_a      # [sampled_rpns, ]\n",
        "    t_y = (gt_boxes[:, 1] - rpn_boxes[:, 1]) / h_a\n",
        "    t_w = torch.log(w_gt / w_a)\n",
        "    t_h = torch.log(h_gt / w_a)\n",
        "    t_vector = torch.cat((t_x.unsqueeze(-1), t_y.unsqueeze(-1), t_w.unsqueeze(-1), t_h.unsqueeze(-1)), dim=-1)\n",
        "\n",
        "    gt = torch.zeros(gt_boxes.shape[0], 4, 4).to(device)\n",
        "    index = torch.LongTensor(range(gt_boxes.shape[0]))\n",
        "    gt[(index, labels)] = t_vector\n",
        "    gt = gt.view(-1, 4*4)\n",
        "    return gt\n",
        "\n",
        "def decode_boxes(reg_out, rpn_boxes):\n",
        "    '''\n",
        "    Decode regressor outputs from t-vectors form to image coordinates form\n",
        "    t_x = (x − x_a)/w_a \n",
        "    t_y = (y − y_a)/h_a \n",
        "    t_w = log(w/w_a) \n",
        "    t_h = log(h/h_a)\n",
        "    x/y, x_a/y_a are top-left corner of boxes\n",
        "    params:\n",
        "        reg_out: [sampled_rpns, 4*4]\n",
        "        rpn_boxes: [sampled_rpns, 4]\n",
        "    returns:\n",
        "        boxes: [sampled_rpns, 4, 4]\n",
        "    '''\n",
        "    x_a = rpn_boxes[:, 0].unsqueeze(-1).expand(-1, 4)\n",
        "    y_a = rpn_boxes[:, 1].unsqueeze(-1).expand(-1, 4)\n",
        "    w_a = (rpn_boxes[:, 2] - rpn_boxes[:, 0]).unsqueeze(-1).expand(-1, 4)\n",
        "    h_a = (rpn_boxes[:, 3] - rpn_boxes[:, 1]).unsqueeze(-1).expand(-1, 4)\n",
        "\n",
        "    reg_out = reg_out.view(-1, 4, 4)\n",
        "    x1 = reg_out[:,:, 0] * w_a + x_a\n",
        "    y1 = reg_out[:,:, 1] * h_a + y_a\n",
        "    w = w_a * torch.exp(reg_out[:,:,2])\n",
        "    h = h_a * torch.exp(reg_out[:,:,3])\n",
        "\n",
        "    x2 = x1 + w\n",
        "    y2 = y1 + h\n",
        "    \n",
        "    boxes = torch.cat((x1.unsqueeze(-1), y1.unsqueeze(-1), x2.unsqueeze(-1), y2.unsqueeze(-1)), dim=-1)\n",
        "    return boxes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyBeu77JzSPW",
        "colab_type": "code",
        "outputId": "1c1f284e-5d7b-4384-a91a-e740ccd15cb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# losses\n",
        "classifier_loss = nn.BCELoss()\n",
        "def regressor_loss(reg_out, reg_target, labels):\n",
        "    '''\n",
        "    Compute smooth_L1_loss for positive rois\n",
        "    params:\n",
        "        reg_out, reg_target: [sampled_rpns, 4*4]\n",
        "        labels: [sampled_rpns,]\n",
        "    '''\n",
        "    l1_loss = nn.SmoothL1Loss()\n",
        "\n",
        "    # Only positive ROIs contribute to the loss.\n",
        "    positive_idx = torch.where(labels > 0)\n",
        "    positive_target = reg_target[positive_idx[0], :]\n",
        "    positive_out = reg_out[positive_idx[0], :]\n",
        "\n",
        "    # Smooth L1 loss\n",
        "    loss = l1_loss(positive_target, positive_out)\n",
        "    return loss\n",
        "\n",
        "# Train for Box Head\n",
        "epochs = 2\n",
        "step = 20\n",
        "num_iter = 0\n",
        "train_test_ratio = 0.7\n",
        "class_losses, regss_losses, total_losses, iterations = [], [], [], []\n",
        "\n",
        "# Models\n",
        "box_model = Boxes().to(device)\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.Adam(box_model.parameters(), lr=0.0001)\n",
        "\n",
        "# Start training\n",
        "for epoch in range(epochs):\n",
        "    class_loss_total = 0.0\n",
        "    regss_loss_total = 0.0\n",
        "\n",
        "    # iterate over each image\n",
        "    # batch size is 128\n",
        "    start = time.time()\n",
        "    for i in range(int(train_test_ratio * len(dataset))):\n",
        "        # if i > 5:\n",
        "        #     break\n",
        "        data = dataset[i]\n",
        "        x, target, p2, _ = prepare_data(data)                       # x: [sampled_rpns, 256*7*7], target: [sampled_rpns, 9]\n",
        "        \n",
        "        # Encode labels\n",
        "        labels = target[:, 4].type(torch.long)\n",
        "        labels_one_hot = F.one_hot(labels, 4)\n",
        "\n",
        "        # Encode boxes to t* vectors\n",
        "        gt_boxes = target[:, 5:]\n",
        "        rpn_boxes = target[:, :4]\n",
        "        boxes_encoded = encode_boxes(gt_boxes, rpn_boxes, labels)   # [sampled_rpns, 4*4]\n",
        "        \n",
        "        # Model outputs\n",
        "        cls_out, reg_out = box_model(x)                             # cls_out: [sampled_rpns, 4], reg_out: [sampled_rpns, 4*4]\n",
        "\n",
        "        # Loss\n",
        "        cls_loss = classifier_loss(cls_out, labels_one_hot.type(torch.float)) / 8\n",
        "        \n",
        "        reg_loss = regressor_loss(reg_out, boxes_encoded, labels)\n",
        "\n",
        "        class_loss_total += cls_loss.item()\n",
        "        regss_loss_total += reg_loss.item()\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        (cls_loss + reg_loss).backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % step == 0:\n",
        "            end = time.time()\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Classifier Loss: {:.4f}, Regressor loss: {:.4f}, Time elapsed: {:.4f}'.format(epoch+1, \n",
        "                                epochs, i+1, int(train_test_ratio*len(dataset)), class_loss_total / (i+1), regss_loss_total / (i+1), end-start))\n",
        "            class_losses.append(class_loss_total / (i+1))\n",
        "            regss_losses.append(regss_loss_total / (i+1))\n",
        "            total_losses.append(class_loss_total / (i+1) + regss_loss_total / (i+1))\n",
        "            iterations.append(num_iter)\n",
        "\n",
        "        num_iter += 1\n",
        "        torch.save(box_model.state_dict(), MODEL_PATH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [20/2285], Classifier Loss: 0.0381, Regressor loss: 0.0297, Time elapsed: 41.9507\n",
            "Epoch [1/2], Step [40/2285], Classifier Loss: 0.0330, Regressor loss: 0.0266, Time elapsed: 81.5289\n",
            "Epoch [1/2], Step [60/2285], Classifier Loss: 0.0328, Regressor loss: 0.0251, Time elapsed: 126.6563\n",
            "Epoch [1/2], Step [80/2285], Classifier Loss: 0.0306, Regressor loss: 0.0226, Time elapsed: 166.9434\n",
            "Epoch [1/2], Step [100/2285], Classifier Loss: 0.0288, Regressor loss: 0.0210, Time elapsed: 208.6540\n",
            "Epoch [1/2], Step [120/2285], Classifier Loss: 0.0275, Regressor loss: 0.0195, Time elapsed: 251.8496\n",
            "Epoch [1/2], Step [140/2285], Classifier Loss: 0.0264, Regressor loss: 0.0184, Time elapsed: 295.3497\n",
            "Epoch [1/2], Step [160/2285], Classifier Loss: 0.0257, Regressor loss: 0.0175, Time elapsed: 340.8671\n",
            "Epoch [1/2], Step [180/2285], Classifier Loss: 0.0247, Regressor loss: 0.0167, Time elapsed: 380.3473\n",
            "Epoch [1/2], Step [200/2285], Classifier Loss: 0.0239, Regressor loss: 0.0160, Time elapsed: 419.8852\n",
            "Epoch [1/2], Step [220/2285], Classifier Loss: 0.0234, Regressor loss: 0.0157, Time elapsed: 464.4451\n",
            "Epoch [1/2], Step [240/2285], Classifier Loss: 0.0228, Regressor loss: 0.0152, Time elapsed: 507.0026\n",
            "Epoch [1/2], Step [260/2285], Classifier Loss: 0.0222, Regressor loss: 0.0150, Time elapsed: 548.0832\n",
            "Epoch [1/2], Step [280/2285], Classifier Loss: 0.0218, Regressor loss: 0.0145, Time elapsed: 589.0620\n",
            "Epoch [1/2], Step [300/2285], Classifier Loss: 0.0213, Regressor loss: 0.0142, Time elapsed: 631.4104\n",
            "Epoch [1/2], Step [320/2285], Classifier Loss: 0.0207, Regressor loss: 0.0140, Time elapsed: 670.3994\n",
            "Epoch [1/2], Step [340/2285], Classifier Loss: 0.0204, Regressor loss: 0.0137, Time elapsed: 711.1989\n",
            "Epoch [1/2], Step [360/2285], Classifier Loss: 0.0201, Regressor loss: 0.0135, Time elapsed: 755.7812\n",
            "Epoch [1/2], Step [380/2285], Classifier Loss: 0.0197, Regressor loss: 0.0132, Time elapsed: 797.4981\n",
            "Epoch [1/2], Step [400/2285], Classifier Loss: 0.0194, Regressor loss: 0.0130, Time elapsed: 839.7792\n",
            "Epoch [1/2], Step [420/2285], Classifier Loss: 0.0191, Regressor loss: 0.0128, Time elapsed: 881.3608\n",
            "Epoch [1/2], Step [440/2285], Classifier Loss: 0.0188, Regressor loss: 0.0127, Time elapsed: 922.7312\n",
            "Epoch [1/2], Step [460/2285], Classifier Loss: 0.0185, Regressor loss: 0.0127, Time elapsed: 961.5660\n",
            "Epoch [1/2], Step [480/2285], Classifier Loss: 0.0183, Regressor loss: 0.0125, Time elapsed: 1002.7998\n",
            "Epoch [1/2], Step [500/2285], Classifier Loss: 0.0181, Regressor loss: 0.0123, Time elapsed: 1042.3736\n",
            "Epoch [1/2], Step [520/2285], Classifier Loss: 0.0178, Regressor loss: 0.0121, Time elapsed: 1083.7217\n",
            "Epoch [1/2], Step [540/2285], Classifier Loss: 0.0175, Regressor loss: 0.0119, Time elapsed: 1121.5382\n",
            "Epoch [1/2], Step [560/2285], Classifier Loss: 0.0174, Regressor loss: 0.0118, Time elapsed: 1161.4999\n",
            "Epoch [1/2], Step [580/2285], Classifier Loss: 0.0172, Regressor loss: 0.0116, Time elapsed: 1204.7086\n",
            "Epoch [1/2], Step [600/2285], Classifier Loss: 0.0171, Regressor loss: 0.0115, Time elapsed: 1246.5465\n",
            "Epoch [1/2], Step [620/2285], Classifier Loss: 0.0169, Regressor loss: 0.0114, Time elapsed: 1286.8432\n",
            "Epoch [1/2], Step [640/2285], Classifier Loss: 0.0169, Regressor loss: 0.0113, Time elapsed: 1329.9706\n",
            "Epoch [1/2], Step [660/2285], Classifier Loss: 0.0167, Regressor loss: 0.0112, Time elapsed: 1371.1635\n",
            "Epoch [1/2], Step [680/2285], Classifier Loss: 0.0165, Regressor loss: 0.0111, Time elapsed: 1414.3208\n",
            "Epoch [1/2], Step [700/2285], Classifier Loss: 0.0164, Regressor loss: 0.0110, Time elapsed: 1455.6850\n",
            "Epoch [1/2], Step [720/2285], Classifier Loss: 0.0162, Regressor loss: 0.0109, Time elapsed: 1497.3990\n",
            "Epoch [1/2], Step [740/2285], Classifier Loss: 0.0160, Regressor loss: 0.0108, Time elapsed: 1539.7111\n",
            "Epoch [1/2], Step [760/2285], Classifier Loss: 0.0159, Regressor loss: 0.0107, Time elapsed: 1579.5490\n",
            "Epoch [1/2], Step [780/2285], Classifier Loss: 0.0157, Regressor loss: 0.0106, Time elapsed: 1620.5508\n",
            "Epoch [1/2], Step [800/2285], Classifier Loss: 0.0156, Regressor loss: 0.0106, Time elapsed: 1662.4155\n",
            "Epoch [1/2], Step [820/2285], Classifier Loss: 0.0155, Regressor loss: 0.0106, Time elapsed: 1701.7047\n",
            "Epoch [1/2], Step [840/2285], Classifier Loss: 0.0153, Regressor loss: 0.0105, Time elapsed: 1742.4894\n",
            "Epoch [1/2], Step [860/2285], Classifier Loss: 0.0152, Regressor loss: 0.0105, Time elapsed: 1783.3640\n",
            "Epoch [1/2], Step [880/2285], Classifier Loss: 0.0151, Regressor loss: 0.0104, Time elapsed: 1823.7346\n",
            "Epoch [1/2], Step [900/2285], Classifier Loss: 0.0150, Regressor loss: 0.0104, Time elapsed: 1864.2010\n",
            "Epoch [1/2], Step [920/2285], Classifier Loss: 0.0149, Regressor loss: 0.0103, Time elapsed: 1906.1842\n",
            "Epoch [1/2], Step [940/2285], Classifier Loss: 0.0148, Regressor loss: 0.0102, Time elapsed: 1948.4773\n",
            "Epoch [1/2], Step [960/2285], Classifier Loss: 0.0147, Regressor loss: 0.0102, Time elapsed: 1987.9219\n",
            "Epoch [1/2], Step [980/2285], Classifier Loss: 0.0146, Regressor loss: 0.0101, Time elapsed: 2029.4418\n",
            "Epoch [1/2], Step [1000/2285], Classifier Loss: 0.0145, Regressor loss: 0.0100, Time elapsed: 2071.3692\n",
            "Epoch [1/2], Step [1020/2285], Classifier Loss: 0.0144, Regressor loss: 0.0100, Time elapsed: 2113.4655\n",
            "Epoch [1/2], Step [1040/2285], Classifier Loss: 0.0143, Regressor loss: 0.0099, Time elapsed: 2155.5481\n",
            "Epoch [1/2], Step [1060/2285], Classifier Loss: 0.0142, Regressor loss: 0.0098, Time elapsed: 2198.7068\n",
            "Epoch [1/2], Step [1080/2285], Classifier Loss: 0.0142, Regressor loss: 0.0098, Time elapsed: 2242.7343\n",
            "Epoch [1/2], Step [1100/2285], Classifier Loss: 0.0142, Regressor loss: 0.0097, Time elapsed: 2287.9308\n",
            "Epoch [1/2], Step [1120/2285], Classifier Loss: 0.0141, Regressor loss: 0.0096, Time elapsed: 2328.8665\n",
            "Epoch [1/2], Step [1140/2285], Classifier Loss: 0.0140, Regressor loss: 0.0096, Time elapsed: 2367.9304\n",
            "Epoch [1/2], Step [1160/2285], Classifier Loss: 0.0139, Regressor loss: 0.0095, Time elapsed: 2412.4510\n",
            "Epoch [1/2], Step [1180/2285], Classifier Loss: 0.0138, Regressor loss: 0.0095, Time elapsed: 2457.5383\n",
            "Epoch [1/2], Step [1200/2285], Classifier Loss: 0.0138, Regressor loss: 0.0094, Time elapsed: 2500.7717\n",
            "Epoch [1/2], Step [1220/2285], Classifier Loss: 0.0137, Regressor loss: 0.0094, Time elapsed: 2540.1966\n",
            "Epoch [1/2], Step [1240/2285], Classifier Loss: 0.0137, Regressor loss: 0.0093, Time elapsed: 2583.9711\n",
            "Epoch [1/2], Step [1260/2285], Classifier Loss: 0.0136, Regressor loss: 0.0093, Time elapsed: 2623.2802\n",
            "Epoch [1/2], Step [1280/2285], Classifier Loss: 0.0136, Regressor loss: 0.0093, Time elapsed: 2664.0672\n",
            "Epoch [1/2], Step [1300/2285], Classifier Loss: 0.0135, Regressor loss: 0.0092, Time elapsed: 2705.6637\n",
            "Epoch [1/2], Step [1320/2285], Classifier Loss: 0.0134, Regressor loss: 0.0092, Time elapsed: 2746.1390\n",
            "Epoch [1/2], Step [1340/2285], Classifier Loss: 0.0133, Regressor loss: 0.0092, Time elapsed: 2787.2674\n",
            "Epoch [1/2], Step [1360/2285], Classifier Loss: 0.0133, Regressor loss: 0.0091, Time elapsed: 2831.6381\n",
            "Epoch [1/2], Step [1380/2285], Classifier Loss: 0.0132, Regressor loss: 0.0091, Time elapsed: 2876.3310\n",
            "Epoch [1/2], Step [1400/2285], Classifier Loss: 0.0132, Regressor loss: 0.0090, Time elapsed: 2919.2440\n",
            "Epoch [1/2], Step [1420/2285], Classifier Loss: 0.0131, Regressor loss: 0.0090, Time elapsed: 2961.5202\n",
            "Epoch [1/2], Step [1440/2285], Classifier Loss: 0.0131, Regressor loss: 0.0090, Time elapsed: 3003.1561\n",
            "Epoch [1/2], Step [1460/2285], Classifier Loss: 0.0130, Regressor loss: 0.0090, Time elapsed: 3041.4619\n",
            "Epoch [1/2], Step [1480/2285], Classifier Loss: 0.0130, Regressor loss: 0.0090, Time elapsed: 3084.2780\n",
            "Epoch [1/2], Step [1500/2285], Classifier Loss: 0.0129, Regressor loss: 0.0089, Time elapsed: 3124.6858\n",
            "Epoch [1/2], Step [1520/2285], Classifier Loss: 0.0129, Regressor loss: 0.0089, Time elapsed: 3165.1821\n",
            "Epoch [1/2], Step [1540/2285], Classifier Loss: 0.0128, Regressor loss: 0.0089, Time elapsed: 3208.6442\n",
            "Epoch [1/2], Step [1560/2285], Classifier Loss: 0.0128, Regressor loss: 0.0088, Time elapsed: 3251.1760\n",
            "Epoch [1/2], Step [1580/2285], Classifier Loss: 0.0128, Regressor loss: 0.0088, Time elapsed: 3293.4243\n",
            "Epoch [1/2], Step [1600/2285], Classifier Loss: 0.0127, Regressor loss: 0.0088, Time elapsed: 3333.7640\n",
            "Epoch [1/2], Step [1620/2285], Classifier Loss: 0.0126, Regressor loss: 0.0087, Time elapsed: 3379.5441\n",
            "Epoch [1/2], Step [1640/2285], Classifier Loss: 0.0126, Regressor loss: 0.0087, Time elapsed: 3419.6297\n",
            "Epoch [1/2], Step [1660/2285], Classifier Loss: 0.0126, Regressor loss: 0.0087, Time elapsed: 3458.2667\n",
            "Epoch [1/2], Step [1680/2285], Classifier Loss: 0.0125, Regressor loss: 0.0087, Time elapsed: 3501.0650\n",
            "Epoch [1/2], Step [1700/2285], Classifier Loss: 0.0125, Regressor loss: 0.0086, Time elapsed: 3543.3488\n",
            "Epoch [1/2], Step [1720/2285], Classifier Loss: 0.0125, Regressor loss: 0.0086, Time elapsed: 3585.2799\n",
            "Epoch [1/2], Step [1740/2285], Classifier Loss: 0.0124, Regressor loss: 0.0086, Time elapsed: 3625.2773\n",
            "Epoch [1/2], Step [1760/2285], Classifier Loss: 0.0124, Regressor loss: 0.0085, Time elapsed: 3663.7178\n",
            "Epoch [1/2], Step [1780/2285], Classifier Loss: 0.0123, Regressor loss: 0.0085, Time elapsed: 3706.0925\n",
            "Epoch [1/2], Step [1800/2285], Classifier Loss: 0.0124, Regressor loss: 0.0085, Time elapsed: 3749.3852\n",
            "Epoch [1/2], Step [1820/2285], Classifier Loss: 0.0123, Regressor loss: 0.0085, Time elapsed: 3793.1822\n",
            "Epoch [1/2], Step [1840/2285], Classifier Loss: 0.0123, Regressor loss: 0.0084, Time elapsed: 3838.1937\n",
            "Epoch [1/2], Step [1860/2285], Classifier Loss: 0.0123, Regressor loss: 0.0084, Time elapsed: 3881.1738\n",
            "Epoch [1/2], Step [1880/2285], Classifier Loss: 0.0122, Regressor loss: 0.0084, Time elapsed: 3924.3884\n",
            "Epoch [1/2], Step [1900/2285], Classifier Loss: 0.0122, Regressor loss: 0.0083, Time elapsed: 3964.8472\n",
            "Epoch [1/2], Step [1920/2285], Classifier Loss: 0.0121, Regressor loss: 0.0083, Time elapsed: 4007.0400\n",
            "Epoch [1/2], Step [1940/2285], Classifier Loss: 0.0121, Regressor loss: 0.0082, Time elapsed: 4050.4055\n",
            "Epoch [1/2], Step [1960/2285], Classifier Loss: 0.0121, Regressor loss: 0.0082, Time elapsed: 4090.3417\n",
            "Epoch [1/2], Step [1980/2285], Classifier Loss: 0.0120, Regressor loss: 0.0082, Time elapsed: 4130.8159\n",
            "Epoch [1/2], Step [2000/2285], Classifier Loss: 0.0120, Regressor loss: 0.0081, Time elapsed: 4170.2606\n",
            "Epoch [1/2], Step [2020/2285], Classifier Loss: 0.0120, Regressor loss: 0.0081, Time elapsed: 4210.1330\n",
            "Epoch [1/2], Step [2040/2285], Classifier Loss: 0.0120, Regressor loss: 0.0081, Time elapsed: 4251.4041\n",
            "Epoch [1/2], Step [2060/2285], Classifier Loss: 0.0119, Regressor loss: 0.0081, Time elapsed: 4289.9639\n",
            "Epoch [1/2], Step [2080/2285], Classifier Loss: 0.0119, Regressor loss: 0.0081, Time elapsed: 4331.6627\n",
            "Epoch [1/2], Step [2100/2285], Classifier Loss: 0.0119, Regressor loss: 0.0080, Time elapsed: 4372.6057\n",
            "Epoch [1/2], Step [2120/2285], Classifier Loss: 0.0118, Regressor loss: 0.0080, Time elapsed: 4411.5629\n",
            "Epoch [1/2], Step [2140/2285], Classifier Loss: 0.0118, Regressor loss: 0.0080, Time elapsed: 4452.7569\n",
            "Epoch [1/2], Step [2160/2285], Classifier Loss: 0.0118, Regressor loss: 0.0080, Time elapsed: 4494.8741\n",
            "Epoch [1/2], Step [2180/2285], Classifier Loss: 0.0117, Regressor loss: 0.0080, Time elapsed: 4535.0240\n",
            "Epoch [1/2], Step [2200/2285], Classifier Loss: 0.0117, Regressor loss: 0.0080, Time elapsed: 4576.1611\n",
            "Epoch [1/2], Step [2220/2285], Classifier Loss: 0.0117, Regressor loss: 0.0079, Time elapsed: 4618.5211\n",
            "Epoch [1/2], Step [2240/2285], Classifier Loss: 0.0116, Regressor loss: 0.0079, Time elapsed: 4658.5095\n",
            "Epoch [1/2], Step [2260/2285], Classifier Loss: 0.0116, Regressor loss: 0.0079, Time elapsed: 4700.0403\n",
            "Epoch [1/2], Step [2280/2285], Classifier Loss: 0.0116, Regressor loss: 0.0079, Time elapsed: 4741.1343\n",
            "Epoch [2/2], Step [20/2285], Classifier Loss: 0.0105, Regressor loss: 0.0059, Time elapsed: 42.6947\n",
            "Epoch [2/2], Step [40/2285], Classifier Loss: 0.0094, Regressor loss: 0.0060, Time elapsed: 82.5871\n",
            "Epoch [2/2], Step [60/2285], Classifier Loss: 0.0100, Regressor loss: 0.0061, Time elapsed: 128.1309\n",
            "Epoch [2/2], Step [80/2285], Classifier Loss: 0.0093, Regressor loss: 0.0058, Time elapsed: 167.9543\n",
            "Epoch [2/2], Step [100/2285], Classifier Loss: 0.0089, Regressor loss: 0.0054, Time elapsed: 209.4669\n",
            "Epoch [2/2], Step [120/2285], Classifier Loss: 0.0094, Regressor loss: 0.0053, Time elapsed: 252.3839\n",
            "Epoch [2/2], Step [140/2285], Classifier Loss: 0.0093, Regressor loss: 0.0052, Time elapsed: 296.2848\n",
            "Epoch [2/2], Step [160/2285], Classifier Loss: 0.0098, Regressor loss: 0.0053, Time elapsed: 341.6387\n",
            "Epoch [2/2], Step [180/2285], Classifier Loss: 0.0096, Regressor loss: 0.0052, Time elapsed: 380.5374\n",
            "Epoch [2/2], Step [200/2285], Classifier Loss: 0.0097, Regressor loss: 0.0053, Time elapsed: 420.0294\n",
            "Epoch [2/2], Step [220/2285], Classifier Loss: 0.0101, Regressor loss: 0.0054, Time elapsed: 465.2751\n",
            "Epoch [2/2], Step [240/2285], Classifier Loss: 0.0100, Regressor loss: 0.0054, Time elapsed: 508.1610\n",
            "Epoch [2/2], Step [260/2285], Classifier Loss: 0.0099, Regressor loss: 0.0055, Time elapsed: 549.6156\n",
            "Epoch [2/2], Step [280/2285], Classifier Loss: 0.0099, Regressor loss: 0.0055, Time elapsed: 591.1590\n",
            "Epoch [2/2], Step [300/2285], Classifier Loss: 0.0098, Regressor loss: 0.0054, Time elapsed: 633.6947\n",
            "Epoch [2/2], Step [320/2285], Classifier Loss: 0.0097, Regressor loss: 0.0053, Time elapsed: 673.0481\n",
            "Epoch [2/2], Step [340/2285], Classifier Loss: 0.0097, Regressor loss: 0.0053, Time elapsed: 714.4111\n",
            "Epoch [2/2], Step [360/2285], Classifier Loss: 0.0097, Regressor loss: 0.0053, Time elapsed: 758.4870\n",
            "Epoch [2/2], Step [380/2285], Classifier Loss: 0.0095, Regressor loss: 0.0052, Time elapsed: 799.3016\n",
            "Epoch [2/2], Step [400/2285], Classifier Loss: 0.0095, Regressor loss: 0.0052, Time elapsed: 841.0380\n",
            "Epoch [2/2], Step [420/2285], Classifier Loss: 0.0096, Regressor loss: 0.0053, Time elapsed: 883.1921\n",
            "Epoch [2/2], Step [440/2285], Classifier Loss: 0.0096, Regressor loss: 0.0052, Time elapsed: 924.0279\n",
            "Epoch [2/2], Step [460/2285], Classifier Loss: 0.0095, Regressor loss: 0.0053, Time elapsed: 962.8688\n",
            "Epoch [2/2], Step [480/2285], Classifier Loss: 0.0095, Regressor loss: 0.0053, Time elapsed: 1003.6399\n",
            "Epoch [2/2], Step [500/2285], Classifier Loss: 0.0095, Regressor loss: 0.0053, Time elapsed: 1043.5831\n",
            "Epoch [2/2], Step [520/2285], Classifier Loss: 0.0094, Regressor loss: 0.0053, Time elapsed: 1085.0332\n",
            "Epoch [2/2], Step [540/2285], Classifier Loss: 0.0093, Regressor loss: 0.0052, Time elapsed: 1123.0668\n",
            "Epoch [2/2], Step [560/2285], Classifier Loss: 0.0094, Regressor loss: 0.0053, Time elapsed: 1163.6433\n",
            "Epoch [2/2], Step [580/2285], Classifier Loss: 0.0093, Regressor loss: 0.0052, Time elapsed: 1206.7575\n",
            "Epoch [2/2], Step [600/2285], Classifier Loss: 0.0093, Regressor loss: 0.0052, Time elapsed: 1248.3155\n",
            "Epoch [2/2], Step [620/2285], Classifier Loss: 0.0093, Regressor loss: 0.0053, Time elapsed: 1288.9864\n",
            "Epoch [2/2], Step [640/2285], Classifier Loss: 0.0093, Regressor loss: 0.0053, Time elapsed: 1332.2100\n",
            "Epoch [2/2], Step [660/2285], Classifier Loss: 0.0093, Regressor loss: 0.0052, Time elapsed: 1373.3958\n",
            "Epoch [2/2], Step [680/2285], Classifier Loss: 0.0092, Regressor loss: 0.0052, Time elapsed: 1415.6347\n",
            "Epoch [2/2], Step [700/2285], Classifier Loss: 0.0093, Regressor loss: 0.0052, Time elapsed: 1456.3126\n",
            "Epoch [2/2], Step [720/2285], Classifier Loss: 0.0092, Regressor loss: 0.0052, Time elapsed: 1498.6788\n",
            "Epoch [2/2], Step [740/2285], Classifier Loss: 0.0092, Regressor loss: 0.0052, Time elapsed: 1540.7329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "levfvnO6pvB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(iterations, class_losses)\n",
        "plt.plot(iterations, regss_losses)\n",
        "plt.plot(iterations, total_losses)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"loss vs epochs\")\n",
        "plt.grid(True)\n",
        "plt.legend([\"Classification Loss\", \"Regression Loss\", \"Whole Loss\"])\n",
        "plt.savefig(\"loss.eps\", format=\"eps\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LfiE53WzXEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def postProcess(reg_out, rpn_boxes, cls_out):\n",
        "    '''\n",
        "    1. Decoding the regressor output from t vector to boxes coordinates\n",
        "    2. Remove invalid boxes (empty and cross boundary)\n",
        "    3. Apply nms with iou=0.5 (independently for each class, except background)\n",
        "    4. If the detections are many (usually 100 are enough),\n",
        "       sort them by some confidence criterion and remove the rest\n",
        "    params:\n",
        "        reg_out: [sampled_rpns, 4, 4]\n",
        "        rpn_boxes: [sampled_rpns, 4]\n",
        "        cls_out: [sampled_rpns, 4]\n",
        "    returns:\n",
        "        keep_top100: tensor[100, 6] in form of [x1,y1,x2,y2,score,class]\n",
        "    '''\n",
        "    # Decode the regressor output\n",
        "    regout_decoded = decode_boxes(reg_out, rpn_boxes)           # [sampled_rpns,4,4]\n",
        "\n",
        "    # Remove empty and cross boundary boxes\n",
        "    idx_negative = torch.where(regout_decoded < 0)\n",
        "    regout_decoded[idx_negative[0], idx_negative[1]] = -1\n",
        "\n",
        "    idx_x1_cross = torch.where(regout_decoded[:,:,0] > 1088)\n",
        "    regout_decoded[idx_x1_cross[0], idx_x1_cross[1]] = -1\n",
        "\n",
        "    idx_x2_cross = torch.where(regout_decoded[:,:,2] > 1088)\n",
        "    regout_decoded[idx_x2_cross[0], idx_x2_cross[1]] = -1\n",
        "\n",
        "    idx_y1_cross = torch.where(regout_decoded[:,:,1] > 800)\n",
        "    regout_decoded[idx_y1_cross[0], idx_y1_cross[1]] = -1\n",
        "\n",
        "    idx_y2_cross = torch.where(regout_decoded[:,:,3] > 800)\n",
        "    regout_decoded[idx_y2_cross[0], idx_y2_cross[1]] = -1\n",
        "\n",
        "    # print(regout_decoded)\n",
        "\n",
        "    keep = torch.tensor([]).to(device)\n",
        "    for i in range(1, 4):\n",
        "        regout_per_cls = regout_decoded[:, i]                   # [sampled_rpns, 4]\n",
        "        regout_positive = torch.where(regout_per_cls[:, 0] > -1)\n",
        "        boxes_per_cls = regout_per_cls[regout_positive[0]]      # [N, 4]\n",
        "        scores_per_cls = cls_out[:, i]\n",
        "        scores_per_cls = scores_per_cls[regout_positive[0]]\n",
        "\n",
        "        # Apply nms for each class\n",
        "        keep_idx = ops.nms(boxes_per_cls, scores_per_cls, 0.5)\n",
        "\n",
        "        keep_boxes = boxes_per_cls[keep_idx]\n",
        "        keep_scores = scores_per_cls[keep_idx]\n",
        "        keep_box_score = torch.cat((keep_boxes, keep_scores.unsqueeze(-1)), dim=-1)   # [N_nms, 5]\n",
        "  \n",
        "        cls = torch.tensor([[i]]).type(torch.float).to(device)\n",
        "        cls = cls.expand_as(keep_scores.unsqueeze(-1))\n",
        "        keep_cls = torch.cat((keep_box_score, cls), dim=-1)     # [N_nms, 6] as [x1,y1,x2,y2,score,class]\n",
        "        keep = torch.cat((keep, keep_cls), dim=0)\n",
        "        \n",
        "    # The maximum detections combined for all classes in a image\n",
        "    # Find the top 100 boxes with scores\n",
        "    _, idx_sorted = torch.sort(keep[:, 4], descending=True)\n",
        "    keep_sorted = keep[idx_sorted]\n",
        "    keep_top100 = keep_sorted[:100]\n",
        "   \n",
        "    return keep_top100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-wqsvbmzeFQ",
        "colab_type": "code",
        "outputId": "45f94ae5-7aeb-4501-fb80-e6a58ba3c3d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        }
      },
      "source": [
        "# Load model\n",
        "box_model_val = Boxes().to(device)\n",
        "box_model_val.eval()\n",
        "\n",
        "checkpoint = torch.load(MODEL_PATH)\n",
        "box_model_val.load_state_dict(checkpoint)\n",
        "\n",
        "total_mAP = 0\n",
        "plot = True\n",
        "with torch.no_grad():\n",
        "\n",
        "    for i in range(int(0.7 * len(dataset)), len(dataset)):\n",
        "        # if i > int(0.7 * len(dataset)) + 5:\n",
        "        #     break\n",
        "        data = dataset[i]\n",
        "        image = data['image']\n",
        "        x, target, p2, _ = prepare_data(data) \n",
        "        rpn_boxes = target[:, :4]\n",
        "\n",
        "        cls_out, reg_out = box_model_val(x)\n",
        "\n",
        "        keep = postProcess(reg_out, rpn_boxes, cls_out)     # [100, 6]\n",
        "\n",
        "        gt_label = data['target']['label'].to(device)\n",
        "        gt_boxes = data['target']['bbox'].to(device)\n",
        "        gt_scores = []\n",
        "\n",
        "        # Compute average precision\n",
        "        for keep_idx in range(100):\n",
        "            output = keep[keep_idx]                         # [x1,y1,x2,y2,score,class]\n",
        "            label = output[5]\n",
        "            # get the ground truth label for each kept proposal\n",
        "            for box_idx in range(gt_boxes.shape[0]):\n",
        "                if label == gt_label[box_idx]:\n",
        "                    iou = compute_iou(gt_boxes[box_idx], output[:4])\n",
        "                    if iou >= 0.3:\n",
        "                        gt_scores.append(1)\n",
        "\n",
        "            if (keep_idx + 1) > len(gt_scores):\n",
        "                gt_scores.append(0)\n",
        "\n",
        "        scores = keep[:, 4].cpu().numpy()\n",
        "        gt_scores = np.array(gt_scores)\n",
        "        mAP = average_precision_score(gt_scores, scores)\n",
        "        if (i+1) % 20 == 0:\n",
        "            print(\"mAP of {}: {}\".format(i, mAP))\n",
        "        if np.isnan(mAP):\n",
        "            continue\n",
        "        total_mAP += mAP\n",
        "        \n",
        "\n",
        "        # # Get each class's AP/recall\n",
        "        # for cls in range(1, 4):\n",
        "        #     cls_idx = torch.where(keep[:, 5] == cls)\n",
        "        #     scores = keep[cls_idx[0]][:, 4].cpu().numpy()\n",
        "        #     gt_score = gt_scores[cls_idx[0].cpu().numpy()]\n",
        "        #     precision, recall, _ = precision_recall_curve(gt_score, scores)\n",
        "        \n",
        "        if plot:\n",
        "            if i < 10:\n",
        "                plt.figure(i)      \n",
        "                draw_reg_out(image.squeeze().cpu(), keep[:20, :4], keep[:20, 5], plt)\n",
        "\n",
        "print(\"Mean Average Precision over all classes: \", total_mAP / ( len(dataset) - int(0.7 * len(dataset))))\n",
        "\n",
        "def draw_reg_out(image, boxes, labels, plt):\n",
        "    '''\n",
        "    image: tensor(3, 800, 1088)\n",
        "    boxes: [n, 4] as form of  [x1,y1,x2,y2]\n",
        "    labels: [n, ]\n",
        "    '''\n",
        "\n",
        "    image = np.transpose(image.numpy(), (1, 2, 0))\n",
        "    image_convert = (image - image.min()) * (1/(image.max() - image.min()) * 255)\n",
        "    plt.imshow(image_convert.astype(np.uint8))\n",
        "    \n",
        "    for i in range(boxes.shape[0]):\n",
        "        # Bounding box\n",
        "        x1, y1, x2, y2 = boxes[i,:]\n",
        "        w = x2 - x1\n",
        "        h = y2 - y1\n",
        "\n",
        "        if labels[i].item() == 1.0:     # 1: vehicle\n",
        "            color = 'r'\n",
        "        elif labels[i].item() == 2.0:   # 2: person\n",
        "            color = 'g'\n",
        "        else:                           # 3: animal\n",
        "            color = 'b'\n",
        "\n",
        "        ax = plt.gca()\n",
        "        rect = patches.Rectangle((x1,y1),w,h,linewidth=2,edgecolor=color,facecolor='none')\n",
        "        ax.add_patch(rect)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mAP of 2299: 0.6137391439205955\n",
            "mAP of 2319: 0.49831853195823783\n",
            "mAP of 2339: 0.2121212121212121\n",
            "mAP of 2359: 0.5588235294117647\n",
            "mAP of 2379: 0.04456928838951311\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py:528: RuntimeWarning: invalid value encountered in true_divide\n",
            "  recall = tps / tps[-1]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mAP of 2399: nan\n",
            "mAP of 2419: 0.9166666666666665\n",
            "mAP of 2439: 1.0\n",
            "mAP of 2459: 0.9797979797979799\n",
            "mAP of 2479: 0.8198412698412698\n",
            "mAP of 2499: 0.8371945828097191\n",
            "mAP of 2519: 0.345985613296964\n",
            "mAP of 2539: 0.7698412698412698\n",
            "mAP of 2559: 0.614768050879162\n",
            "mAP of 2579: 0.03225806451612903\n",
            "mAP of 2599: 0.6297222222222222\n",
            "mAP of 2619: 0.8125\n",
            "mAP of 2639: 1.0\n",
            "mAP of 2659: 0.8809523809523809\n",
            "mAP of 2679: 0.5625\n",
            "mAP of 2699: 0.6245590828924161\n",
            "mAP of 2719: 0.6319444444444444\n",
            "mAP of 2739: 0.5863636363636364\n",
            "mAP of 2759: 0.47619047619047616\n",
            "mAP of 2779: 0.7756410256410257\n",
            "mAP of 2799: 0.7484126984126984\n",
            "mAP of 2819: 0.9388095238095239\n",
            "mAP of 2839: 0.7669491525423728\n",
            "mAP of 2859: 0.6851851851851851\n",
            "mAP of 2879: 0.7875\n",
            "mAP of 2899: 0.7951789411416739\n",
            "mAP of 2919: 0.3321328531412565\n",
            "mAP of 2939: 0.7231149622010966\n",
            "mAP of 2959: 0.8055555555555556\n",
            "mAP of 2979: 0.46039890552085677\n",
            "mAP of 2999: 0.5757911860396332\n",
            "mAP of 3019: 0.41293023915974736\n",
            "mAP of 3039: 0.7638513867085296\n",
            "mAP of 3059: 0.5775373751032981\n",
            "mAP of 3079: 0.5577777777777778\n",
            "mAP of 3099: 0.699023199023199\n",
            "mAP of 3119: 0.7284503091732009\n",
            "mAP of 3139: 0.644987012987013\n",
            "mAP of 3159: 0.3088235294117647\n",
            "mAP of 3179: 0.5878003003003003\n",
            "mAP of 3199: 0.44786885055872394\n",
            "mAP of 3219: 0.681592039800995\n",
            "mAP of 3239: 0.3322000711706594\n",
            "mAP of 3259: 0.21153381642512076\n",
            "Mean Average Precision over all classes:  0.6552387388198944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C4X4Jwhze7E",
        "colab_type": "code",
        "outputId": "64ea0047-4a60-4293-93e3-594d420a3c22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "# Train for Box Head\n",
        "epochs = 2\n",
        "step = 20\n",
        "num_iter = 0\n",
        "train_test_ratio = 0.7\n",
        "class_losses, regss_losses, masks_losses, total_losses, iterations = [], [], [], [], []\n",
        "\n",
        "# Models\n",
        "box_model = Boxes().to(device)\n",
        "mask_model = Masks().to(device)\n",
        "\n",
        "# optimizer\n",
        "optimizer_box = optim.Adam(box_model.parameters(), lr=0.0001)\n",
        "optimizer_mask = optim.Adam(mask_model.parameters(), lr=0.0001)\n",
        "\n",
        "# Loss\n",
        "compute_mask_loss = nn.BCELoss()\n",
        "\n",
        "# Start training\n",
        "for epoch in range(epochs):\n",
        "    class_loss_total = 0.0\n",
        "    regss_loss_total = 0.0\n",
        "    masks_loss_total = 0.0\n",
        "\n",
        "    # iterate over each image\n",
        "    # batch size is 128\n",
        "    start = time.time()\n",
        "    for i in range(int(train_test_ratio * len(dataset))):\n",
        "        # if i > 1:\n",
        "        #     break\n",
        "        data = dataset[i]\n",
        "        x, target, p2, mask_target = prepare_data(data)             # x: [sampled_rpns, 256*7*7], target: [sampled_rpns, 9]\n",
        "        # Encode labels\n",
        "        labels = target[:, 4].type(torch.long)\n",
        "        labels_one_hot = F.one_hot(labels, 4)\n",
        "\n",
        "        # Encode boxes to t* vectors\n",
        "        gt_boxes = target[:, 5:]\n",
        "        rpn_boxes = target[:, :4]\n",
        "        boxes_encoded = encode_boxes(gt_boxes, rpn_boxes, labels)   # [sampled_rpns, 4*4]\n",
        "        \n",
        "        # Box Model\n",
        "        cls_out, reg_out = box_model(x)                             # cls_out: [sampled_rpns, 4], reg_out: [sampled_rpns, 4*4]\n",
        "        keep = postProcess(reg_out, rpn_boxes, cls_out)             # [100, 6]\n",
        "\n",
        "        # Mask model\n",
        "        detections = keep[:, :4]                                    # [100, 4] in image coordinates\n",
        "        roi_align = ROIAlign()\n",
        "        mask_rois = roi_align(p2, [detections/4], [14,14])          # [100, 256, 14, 14]\n",
        "        mask_out = mask_model(mask_rois)                            # [100, 4, 28, 28]\n",
        "        if mask_out.shape[0] < mask_target.shape[0]:                # result of keep might be <100\n",
        "            mask_target = mask_target[:mask_out.shape[0]]\n",
        "        # remove background\n",
        "        # mask_out_rm_bg = mask_out[:, 1:4, :, :]\n",
        "\n",
        "        # Loss\n",
        "        cls_loss = classifier_loss(cls_out, labels_one_hot.type(torch.float))\n",
        "        reg_loss = regressor_loss(reg_out, boxes_encoded, labels)\n",
        "        mask_loss = compute_mask_loss(mask_out, mask_target)\n",
        "\n",
        "        class_loss_total += cls_loss.item()\n",
        "        regss_loss_total += reg_loss.item()\n",
        "        masks_loss_total += mask_loss.item()\n",
        "\n",
        "        # backward\n",
        "        optimizer_box.zero_grad()\n",
        "        optimizer_mask.zero_grad()\n",
        "        (cls_loss + reg_loss + mask_loss).backward()\n",
        "        optimizer_box.step()\n",
        "        optimizer_mask.step()\n",
        "\n",
        "        if (i+1) % step == 0:\n",
        "            end = time.time()\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Classifier Loss: {:.4f}, Regressor loss: {:.4f}, Mask loss: {:.4f}, Time elapsed: {:.4f}'.format(epoch+1, \n",
        "                                epochs, i+1, int(train_test_ratio*len(dataset)), \n",
        "                                class_loss_total / (i+1), regss_loss_total / (i+1), \n",
        "                                masks_loss_total / (i+1), end-start))\n",
        "            \n",
        "            class_losses.append(class_loss_total / (i+1))\n",
        "            regss_losses.append(regss_loss_total / (i+1))\n",
        "            masks_losses.append(masks_loss_total / (i+1))\n",
        "            total_losses.append(class_loss_total / (i+1) + regss_loss_total / (i+1) + masks_loss_total / (i+1))\n",
        "            iterations.append(num_iter)\n",
        "            torch.save({'box_model':box_model.state_dict(), 'mask_model': mask_model.state_dict()}, MASK_MODEL_PATH)\n",
        "\n",
        "        num_iter += 1\n",
        "\n",
        "        torch.save({'box_model':box_model.state_dict(), 'mask_model': mask_model.state_dict()}, MASK_MODEL_PATH)\n",
        "\n",
        "torch.save({'box_model':box_model.state_dict(), 'mask_model': mask_model.state_dict()}, MASK_MODEL_PATH)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(iterations, class_losses)\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"classification loss\")\n",
        "plt.title(\"train loss (classifier) - iteration\")\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(iterations, regss_losses)\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"regression loss\")\n",
        "plt.title(\"train loss (regressor) - iteration\")\n",
        "\n",
        "plt.figure(3)\n",
        "plt.plot(iterations, masks_losses)\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"mask loss\")\n",
        "plt.title(\"train loss (mask) - iteration\")\n",
        "\n",
        "plt.figure(4)\n",
        "plt.plot(iterations, total_losses)\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"total loss\")\n",
        "plt.title(\"train loss (total) - iteration\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/6], Step [20/2285], Classifier Loss: 0.2794, Regressor loss: 0.0259, Mask loss: 0.5539, Time elapsed: 48.1714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-eb08df7d41e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0moptimizer_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0moptimizer_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mcls_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreg_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmask_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0moptimizer_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0moptimizer_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}